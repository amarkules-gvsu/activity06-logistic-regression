---
title: "Activity 6 - Logistic Regression"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)

```

# Day 1 Logistic Regression

```{r}
(resume = read_csv('https://www.openintro.org/data/csv/resume.csv'))
```

### Is this an observational study or an experiment? Explain.

Experiment, the resumes are the same except for the names that the researchers are testing.

### The variable of interest is received_callback. What type of variable is this? What do the values represent?

categorical, whether the person was called back for an interview.

### For received_callback, create an appropriate data visualization using {ggplot2}. Be sure to provide more descriptive labels (both axes labels and value labels - there are many ways to do this) as well as an appropriate title.

```{r}
ggplot(resume) +
  aes(x = factor(received_callback)) +
  geom_bar() +
  scale_x_discrete(labels=c("No", "Yes")) +
  labs(x="Recieved Callback?", y="Count", title = "Resume Callback Likelihood")
```

### Below, I provide you with a numerical summary table that should reiterate (i.e., provides numerical values) your plot in (3). Write the code to produce this table.

```{r}
resume %>%
  mutate(received_callback_txt=ifelse(received_callback == 1, 'Yes', 'No')) %>%
  group_by(received_callback_txt) %>%
  summarize(n = n(), percent = (n()/nrow(resume)) * 100)
```

### What is the probability that a randomly selected résumé/person will be called back?

8.05%

### What are the odds that a randomly selected résumé/person will be called back?

```{r}
0.9195/0.0805
```

1 in 12.42236

To keep things simpler, we will first explore a logistic regression model with a two-level categorical explanatory variable: `race` - the inferred race associated to the first name on the résumé. Below is a two-way table (also known as a contingency table or crosstable), where the rows are the response variable levels, the columns are the explanatory variable levels, and the cells are the percent (and number of in parentheses). Note that the values in each column add to 100%.

| received_callback | Black        | White        |
|-------------------|--------------|--------------|
| No                | 93.55 (2278) | 90.35 (2200) |
| Yes               | 6.45 (157)   | 9.65 (235)   |

Using the above table, answer the following question:

### What is the probability that a randomly selected résumé/person perceived as Black will be called back?

6.45%

### What are the odds that a randomly selected résumé/person perceived as Black will be called back?

```{r}
0.9355/0.0645
```

1 in 14.5

```{r}
# The {tidymodels} method for logistic regression requires that the response be a factor variable
resume <- resume %>% 
  mutate(received_callback = as.factor(received_callback))

resume_mod <- logistic_reg() %>%
  set_engine("glm") %>%
  fit(received_callback ~ race, data = resume, family = "binomial")

tidy(resume_mod) %>% 
  knitr::kable(digits = 3)
```

### Write the estimated regression equation. Round to 3 digits.

logit(received_callback) = -2.675 + 0.438\*racewhite

### Using your equation in (8), write the simplified estimated regression equation corresponding to résumés/persons perceived as Black. Round to 3 digits.

logit(received_callback) = -2.675

### What are the log-odds that they will be called back?

-2.675

### What are the odds that they will be called back? How does this related back to your answer from (7)? Hint: In (9) you obtained the log-odds (i.e., the natural log-odds). How can you back-transform this value to obtain the odds?

```{r}
1/exp(-2.675)
exp(-2.675)
```

odds are 1 in 14.51235 or 0.06890683

### What is the probability that will be called back? How does this related back to your answer from (6)? Hint Use the odds to calculate this value.

```{r}
exp(-2.675)/(1+exp(-2.675)) * 100
```

6.44% - mat

# Day 2 Multiple Logistic Regression

```{r}
resume_select <- resume %>% 
  rename(sex = gender) %>% 
  filter(job_city == "Chicago") %>% 
  mutate(race = case_when(
         race == "white" ~ "White",
         TRUE ~ "Black" 
       ),
       sex = case_when(
         sex == "f" ~ "female",
         TRUE ~ "male"
       )) %>% 
  select(received_callback, years_experience, race, sex)
```


### Explain what six things the above code does in the context of this problem.

It creates a new dataset based on the original resume data, with the gender variable renamed to sex, the records filtered to only those where the job is in chicago, makes the race values title cased, spells out the sex values, and selects only the received_callback, years_experience, race, and sex columns.

### Create a new R code chunk and create an appropriate data visualization to explore the relationship between received_callback and each of the explanatory variables, then run your code chunk or knit your document.

```{r}
GGally::ggpairs(resume_select)
```
way more female applications submitted than male... is that right?

```{r}
resume %>%
  group_by(gender) %>%
  summarize(n=n())
```

guess so, seems like a potential issue but there are enough of each that it shouldn't bee a problem

```{r}
mult_log_mod <- glm(received_callback ~ years_experience + race + sex, data = resume_select, family = "binomial")

tidy(mult_log_mod)
```

```{r}
tidy(mult_log_mod, exponentiate = TRUE) %>% 
  knitr::kable(digits = 3)
```

### Interpret the estimated coefficient for years_experience.

for every year of experience, we expect the odds of an applicant in chicago getting a call back to increase by 1.046.

```{r}
# To store residuals and create row number variable
mult_log_aug <- augment(mult_log_mod, type.predict = "response", 
                      type.residuals = "deviance") %>% 
                      mutate(id = row_number())

# Plot residuals vs fitted values
ggplot(data = mult_log_aug, aes(x = .fitted, y = .resid)) + 
geom_point() + 
geom_hline(yintercept = 0, color = "red") + 
labs(x = "Fitted values", 
     y = "Deviance residuals", 
     title = "Deviance residuals vs. fitted")

# Plot residuals vs row number
ggplot(data = mult_log_aug, aes(x = id, y = .resid)) + 
geom_point() + 
geom_hline(yintercept = 0, color = "red") + 
labs(x = "id", 
     y = "Deviance residuals", 
     title = "Deviance residuals vs. id")
```

# Day 3 - Multinomial Logistic Regression

